# Generated by Django 4.2.19 on 2025-12-06 03:50

from django.conf import settings
import django.core.validators
from django.db import migrations, models
import django.db.models.deletion


class Migration(migrations.Migration):

    dependencies = [
        ('contenttypes', '0002_remove_content_type_name'),
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
        ('active_interview_app', '0018_auditlog'),
    ]

    operations = [
        migrations.CreateModel(
            name='BiasTermLibrary',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('term', models.CharField(help_text='The biased term or phrase (e.g., "cultural fit")', max_length=100)),
                ('category', models.CharField(choices=[('age', 'Age-related'), ('gender', 'Gender-related'), ('race', 'Race/Ethnicity'), ('disability', 'Disability'), ('appearance', 'Physical Appearance'), ('family', 'Family Status'), ('other', 'Other Bias')], help_text='Category of bias this term represents', max_length=20)),
                ('pattern', models.CharField(help_text='Regex pattern for detecting this term and its variations. Example: \\b(cultural fit|culture fit)\\b', max_length=500)),
                ('explanation', models.TextField(help_text='Explanation of why this term is problematic. Shown in tooltip to educate users.')),
                ('neutral_alternatives', models.JSONField(default=list, help_text='List of neutral alternative phrasings. Example: ["team collaboration skills", "alignment with company values"]')),
                ('severity', models.IntegerField(choices=[(1, 'Warning'), (2, 'Blocking')], default=1, help_text='Severity level: WARNING (1) shows warning but allows save, BLOCKING (2) prevents save until resolved')),
                ('is_active', models.BooleanField(default=True, help_text='Whether this term is actively checked during bias detection')),
                ('created_at', models.DateTimeField(auto_now_add=True)),
                ('updated_at', models.DateTimeField(auto_now=True)),
                ('detection_count', models.IntegerField(default=0, help_text='Number of times this term has been detected in feedback')),
                ('created_by', models.ForeignKey(blank=True, help_text='Admin who added this term', null=True, on_delete=django.db.models.deletion.SET_NULL, related_name='created_bias_terms', to=settings.AUTH_USER_MODEL)),
            ],
            options={
                'verbose_name': 'Bias Term',
                'verbose_name_plural': 'Bias Term Library',
                'ordering': ['category', 'term'],
                'indexes': [models.Index(fields=['category', 'is_active'], name='active_inte_categor_ffb268_idx'), models.Index(fields=['is_active', 'severity'], name='active_inte_is_acti_f4da21_idx')],
            },
        ),
        migrations.CreateModel(
            name='BiasAnalysisResult',
            fields=[
                ('id', models.BigAutoField(auto_created=True, primary_key=True, serialize=False, verbose_name='ID')),
                ('object_id', models.CharField(help_text='ID of the specific object (supports both integer IDs and UUIDs)', max_length=255)),
                ('flagged_terms', models.JSONField(blank=True, default=list, help_text='List of detected bias terms with details. Structure: [{"term": str, "category": str, "severity": int, "positions": [int], "suggestions": [str]}, ...]')),
                ('bias_score', models.FloatField(blank=True, help_text='Overall bias probability score (0.0 = clean, 1.0 = highly biased)', null=True, validators=[django.core.validators.MinValueValidator(0.0), django.core.validators.MaxValueValidator(1.0)])),
                ('severity_level', models.CharField(choices=[('CLEAN', 'Clean'), ('LOW', 'Low'), ('MEDIUM', 'Medium'), ('HIGH', 'High')], default='CLEAN', help_text='Overall severity assessment', max_length=10)),
                ('total_flags', models.IntegerField(default=0, help_text='Total number of bias flags detected')),
                ('blocking_flags', models.IntegerField(default=0, help_text='Number of blocking-severity flags (prevent save)')),
                ('warning_flags', models.IntegerField(default=0, help_text='Number of warning-severity flags (allow save with acknowledgment)')),
                ('saved_with_warnings', models.BooleanField(default=False, help_text='Whether the user chose to save despite warnings')),
                ('user_acknowledged', models.BooleanField(default=False, help_text='Whether the user acknowledged the bias warnings')),
                ('analyzed_at', models.DateTimeField(auto_now_add=True)),
                ('feedback_text_hash', models.CharField(blank=True, help_text='Hash of analyzed text (for detecting re-analysis of same content)', max_length=64)),
                ('content_type', models.ForeignKey(help_text='Type of model this analysis is for (InvitedInterview or ExportableReport)', on_delete=django.db.models.deletion.CASCADE, to='contenttypes.contenttype')),
            ],
            options={
                'verbose_name': 'Bias Analysis Result',
                'verbose_name_plural': 'Bias Analysis Results',
                'ordering': ['-analyzed_at'],
                'indexes': [models.Index(fields=['content_type', 'object_id'], name='active_inte_content_d43b47_idx'), models.Index(fields=['severity_level', 'analyzed_at'], name='active_inte_severit_24577b_idx')],
            },
        ),
    ]
