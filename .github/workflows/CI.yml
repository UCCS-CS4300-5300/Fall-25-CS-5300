name: Continuous Integration

on:
  push:
    branches:
      - main
    paths:
      - 'active_interview_backend/**'
      - 'scripts/**'
      - 'docker-compose*.yml'
      - 'railway.toml'
      - '.github/workflows/CI.yml'
  pull_request:
    branches:
      - main
    paths:
      - 'active_interview_backend/**'
      - 'scripts/**'
      - 'docker-compose*.yml'
      - 'railway.toml'
      - '.github/workflows/CI.yml'
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.12'
  CHROME_VERSION: 135.0.7049.52-1
  CHROMEDRIVER_VERSION: 135.0.7049.52
  RETENTION_DAYS: 14

jobs:
  lint:
    name: Run Linting
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install Linters
      run: pip install flake8 djlint

    - name: Lint Python
      if: always()
      run: |
        {
          echo '# Python Lint Output'
          echo '```'
          flake8 --config active_interview_backend/.flake8 . || true
          echo '```'
        } | tee -a $GITHUB_STEP_SUMMARY

    - name: Lint Templates
      if: always()
      run: |
        {
          echo '# Template Lint Output'
          echo '```'
          djlint --configuration active_interview_backend/djlint.toml active_interview_backend/active_interview_app/templates/ --lint || true
          echo '```'
        } | tee -a $GITHUB_STEP_SUMMARY

  security:
    name: Security Scan
    runs-on: ubuntu-latest
    needs: [lint]

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r active_interview_backend/requirements.txt
        pip install safety bandit

    - name: Run Security Checks
      run: |
        {
          echo '# Security Scan Results'
          echo '## Safety Check (Dependency Vulnerabilities)'
          echo '```'
          safety check || true
          echo '```'
          echo ''
          echo '## Bandit Scan (Code Security Issues)'
          echo '```'
          bandit -r . --exclude './.git' -f screen || true
          echo '```'
        } >> $GITHUB_STEP_SUMMARY
        bandit -r . --exclude './.git' -f json -o bandit-report.json || true

    - name: Upload Security Reports
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: security-reports
        path: bandit-report.json
        retention-days: ${{ env.RETENTION_DAYS }}

  test:
    name: Run Tests & Coverage Validation
    runs-on: ubuntu-latest
    needs: [security]

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure Environment
      run: |
        echo 'DJANGO_SECRET_KEY=${{ secrets.DJANGO_SECRET_KEY }}' > .env
        echo 'OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY }}' >> .env

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Build Docker image with GitHub Actions cache
      uses: docker/build-push-action@v6
      with:
        context: active_interview_backend
        target: ci
        load: true
        tags: fall-25-cs-5300-backend:latest
        cache-from: type=gha,scope=build-cache
        cache-to: type=gha,mode=max,scope=build-cache

    - name: Start Containers
      run: docker compose -f docker-compose.prod.yml up -d --no-build

    - name: Wait for Container to be Ready
      run: |
        echo "Waiting for django container to be ready..."
        echo "Container is starting and running migrations + collectstatic via paracord_runner.sh..."

        # Retry loop with timeout
        MAX_RETRIES=30
        RETRY_INTERVAL=2
        COUNTER=0

        while [ $COUNTER -lt $MAX_RETRIES ]; do
          if docker exec django python -c "import django; django.setup()" 2>/dev/null; then
            echo "‚úì Container is ready! (took $((COUNTER * RETRY_INTERVAL)) seconds)"
            exit 0
          fi

          COUNTER=$((COUNTER + 1))
          if [ $COUNTER -lt $MAX_RETRIES ]; then
            echo "Waiting for container... (attempt $COUNTER/$MAX_RETRIES)"
            sleep $RETRY_INTERVAL
          fi
        done

        echo "‚ùå Container failed to become ready after $((MAX_RETRIES * RETRY_INTERVAL)) seconds"
        echo "Container status:"
        docker ps -a | grep django || echo "Container not found!"
        echo "Container logs:"
        docker logs django
        exit 1

    - name: Run Django Tests
      if: always()
      run: |
        set +e  # Don't exit on error
        docker exec django coverage run manage.py test -v 2 > dj_test_out.txt 2>&1
        TEST_EXIT_CODE=$?
        set -e  # Re-enable exit on error

        echo "Test exit code: $TEST_EXIT_CODE"
        echo $TEST_EXIT_CODE > test_exit_code.txt

        # Display the output
        cat dj_test_out.txt

        exit 0  # Don't fail yet, let other steps run

    - name: Display Test Results Summary
      if: always()
      run: |
        # Verify test output file exists
        if [ ! -f dj_test_out.txt ]; then
          echo "‚ö†Ô∏è WARNING: dj_test_out.txt not found!"
          ls -la
          exit 1
        fi

        echo "Test output file size: $(wc -l < dj_test_out.txt) lines"

        # Read the exit code
        if [ -f test_exit_code.txt ]; then
          EXIT_CODE=$(cat test_exit_code.txt)
        else
          EXIT_CODE="unknown"
        fi

        echo "========================================="
        echo "TEST RESULTS SUMMARY"
        echo "========================================="
        echo ""
        echo "Exit Code: $EXIT_CODE"
        echo ""

        # Extract and display test summary
        grep -E "Ran [0-9]+ tests? in" dj_test_out.txt || echo "No test summary found"
        grep -E "^(OK|FAILED)" dj_test_out.txt || echo "Unknown result"
        echo ""

        # Count errors and failures
        ERROR_COUNT=$(grep -E "^ERROR:" dj_test_out.txt 2>/dev/null | wc -l)
        FAIL_COUNT=$(grep -E "^FAIL:" dj_test_out.txt 2>/dev/null | wc -l)

        echo "Errors: $ERROR_COUNT"
        echo "Failures: $FAIL_COUNT"
        echo ""

        # Show list of failed tests if any
        if [ "$EXIT_CODE" -ne 0 ] 2>/dev/null; then
          echo "‚ùå TESTS FAILED"
          echo "========================================="
          echo "FAILED TESTS:"
          echo "========================================="
          grep -E "^(ERROR|FAIL):" dj_test_out.txt 2>/dev/null || echo "No errors or failures found in output (check test output above)"
        else
          echo "‚úÖ ALL TESTS PASSED!"
        fi
        echo ""
        echo "========================================="

    - name: Generate Coverage Report
      if: always()
      run: docker exec django coverage report -m | tee coverage_report.txt

    - name: Check Coverage >= 80%
      if: always()
      run: |
        set -o pipefail
        chmod +x ./scripts/check-coverage.sh
        ./scripts/check-coverage.sh coverage_report.txt | tee coverage_verdict.txt

    - name: Create Step Summary
      if: always()
      run: |
        {
          echo '# Coverage Report'
          echo '```'
          cat coverage_report.txt
          echo '```'
          cat coverage_verdict.txt
          echo ''
          echo '# Test Output'
          echo '```'
          cat dj_test_out.txt
          echo '```'
        } >> $GITHUB_STEP_SUMMARY

    - name: Upload Coverage Report
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: coverage-report
        path: coverage_report.txt
        retention-days: ${{ env.RETENTION_DAYS }}

    - name: Cleanup Docker Resources
      if: always()
      run: |
        echo "Cleaning up Docker containers and images..."
        docker compose -f docker-compose.prod.yml down -v || true
        docker system prune -f || true

    - name: Fail if Tests Failed
      if: always()
      run: |
        if [ -f test_exit_code.txt ]; then
          EXIT_CODE=$(cat test_exit_code.txt)
          if [ "$EXIT_CODE" -ne 0 ]; then
            echo "‚ùå Tests failed with exit code $EXIT_CODE"
            exit 1
          else
            echo "‚úÖ All tests passed!"
          fi
        else
          echo "‚ö†Ô∏è No test exit code found, assuming failure"
          exit 1
        fi

  ai-review:
    name: AI Code Review
    runs-on: ubuntu-latest
    needs: [test]

    steps:
    - name: Checkout Code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install OpenAI
      run: pip install openai

    - name: Calculate Git Diff
      run: |
        if [[ "${{ github.event_name }}" == "push" ]]; then
          if [[ "${{ github.event.before }}" == "0000000000000000000000000000000000000000" ]]; then
            git diff "origin/main" "${{ github.sha }}" > "${{ runner.temp }}/changes.diff"
          else
            git diff "${{ github.event.before }}" "${{ github.event.after }}" > "${{ runner.temp }}/changes.diff"
          fi
        elif [[ "${{ github.event_name }}" == "pull_request" ]]; then
          git diff "${{ github.event.pull_request.base.sha }}" "${{ github.event.pull_request.head.sha }}" > "${{ runner.temp }}/changes.diff"
        fi
        cat "${{ runner.temp }}/changes.diff"

    - name: Run AI Code Review
      run: python scripts/ai-review.py ${{ runner.temp }}/changes.diff
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}

    - name: Upload AI Code Review Report
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: ai-code-review-report
        path: review-*.md
        retention-days: ${{ env.RETENTION_DAYS }}

    - name: Upload Token Usage Data
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: ai-review-token-data
        path: temp/token_usage_*.json
        retention-days: ${{ env.RETENTION_DAYS }}

    - name: Post AI Report PR Comment
      if: github.event_name == 'pull_request'
      uses: mshick/add-pr-comment@v2
      with:
        message-id: ai-code-review-report
        message-path: review-*.md

    - name: Add Review to Summary
      if: always()
      run: cat review-*.md >> $GITHUB_STEP_SUMMARY

  loc-metrics:
    name: Lines of Code Metrics
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'

    steps:
    - name: Checkout Code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Calculate Git Diff
      run: |
        # Calculate diff from the previous commit to current for the merge
        if [[ "${{ github.event.before }}" == "0000000000000000000000000000000000000000" ]]; then
          # First push to branch - compare against HEAD~1
          git diff "HEAD~1" "${{ github.sha }}" > "${{ runner.temp }}/changes.diff"
        else
          # Normal merge - compare before and after
          git diff "${{ github.event.before }}" "${{ github.event.after }}" > "${{ runner.temp }}/changes.diff"
        fi

    - name: Calculate LOC Metrics
      run: |
        DIFF_FILE="${{ runner.temp }}/changes.diff"

        # Count lines added (lines starting with + but not +++)
        LINES_ADDED=$(grep -E '^\+[^+]' "$DIFF_FILE" | wc -l || echo "0")

        # Count lines removed/changed (lines starting with - but not ---)
        LINES_REMOVED=$(grep -E '^\-[^-]' "$DIFF_FILE" | wc -l || echo "0")

        # Calculate net change
        NET_CHANGE=$((LINES_ADDED - LINES_REMOVED))

        # Save metrics to file
        {
          echo "LINES_ADDED=$LINES_ADDED"
          echo "LINES_REMOVED=$LINES_REMOVED"
          echo "NET_CHANGE=$NET_CHANGE"
        } > "${{ runner.temp }}/loc_metrics.txt"

        # Display metrics
        cat "${{ runner.temp }}/loc_metrics.txt"

    - name: Create LOC Metrics Summary
      if: always()
      run: |
        source "${{ runner.temp }}/loc_metrics.txt"

        {
          echo '# Lines of Code Metrics'
          echo ''
          echo '## Summary'
          echo "- **Lines Added:** $LINES_ADDED"
          echo "- **Lines Changed/Removed:** $LINES_REMOVED"
          echo "- **Net Change:** $NET_CHANGE"
          echo ''
          echo '## Details'
          if [ $NET_CHANGE -gt 0 ]; then
            echo "‚úÖ Net addition of $NET_CHANGE lines"
          elif [ $NET_CHANGE -lt 0 ]; then
            echo "‚ôªÔ∏è Net reduction of $((NET_CHANGE * -1)) lines"
          else
            echo "‚ûñ No net change in lines of code"
          fi
          echo ''
          echo '---'
          echo '*Metrics calculated from changes merged into main*'
        } >> $GITHUB_STEP_SUMMARY

    - name: Upload LOC Metrics
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: loc-metrics
        path: ${{ runner.temp }}/loc_metrics.txt
        retention-days: ${{ env.RETENTION_DAYS }}

  cleanup:
    name: Cleanup & Archive
    runs-on: ubuntu-latest
    needs: [lint, security, test, ai-review, loc-metrics]
    if: always()

    steps:
    - name: Download Coverage Report
      uses: actions/download-artifact@v4
      continue-on-error: true
      with:
        name: coverage-report
        path: artifacts/coverage-report

    - name: Download Security Reports
      uses: actions/download-artifact@v4
      continue-on-error: true
      with:
        name: security-reports
        path: artifacts/security-reports

    - name: Download AI Code Review Report
      uses: actions/download-artifact@v4
      continue-on-error: true
      with:
        name: ai-code-review-report
        path: artifacts/ai-code-review-report

    - name: Download LOC Metrics
      uses: actions/download-artifact@v4
      continue-on-error: true
      with:
        name: loc-metrics
        path: artifacts/loc-metrics

    - name: Create Essential Files Archive
      run: |
        echo "Creating archive of essential files..."
        mkdir -p essential-archive

        # Copy coverage reports if they exist
        if [ -d "artifacts/coverage-report" ]; then
          cp -r artifacts/coverage-report essential-archive/
        fi

        # Copy security reports if they exist
        if [ -d "artifacts/security-reports" ]; then
          cp -r artifacts/security-reports essential-archive/
        fi

        # Copy AI review reports if they exist
        if [ -d "artifacts/ai-code-review-report" ]; then
          cp -r artifacts/ai-code-review-report essential-archive/
        fi

        # Copy LOC metrics if they exist
        if [ -d "artifacts/loc-metrics" ]; then
          cp -r artifacts/loc-metrics essential-archive/
        fi

        # Create timestamp file
        echo "Archive created at: $(date)" > essential-archive/archive-info.txt
        echo "Workflow run: ${{ github.run_id }}" >> essential-archive/archive-info.txt
        echo "Commit: ${{ github.sha }}" >> essential-archive/archive-info.txt

        # Create compressed archive
        tar -czf essential-files-${{ github.run_id }}.tar.gz essential-archive/
        ls -lh essential-files-${{ github.run_id }}.tar.gz

    - name: Upload Essential Archive
      uses: actions/upload-artifact@v4
      with:
        name: essential-archive
        path: essential-files-${{ github.run_id }}.tar.gz
        retention-days: 30

    - name: Cleanup Summary
      run: |
        {
          echo '# Cleanup & Archive Summary'
          echo ''
          echo '## Archived Files'
          echo '```'
          ls -lh artifacts/ || echo "No artifacts found"
          echo '```'
          echo ''
          echo '## Essential Archive Created'
          echo '- Archive: essential-files-${{ github.run_id }}.tar.gz'
          echo '- Retention: 30 days'
          echo '- Workflow Run: ${{ github.run_id }}'
          echo ''
          echo '## Cleanup Status'
          echo '- Temporary artifacts cleaned up'
          echo '- Essential files archived and uploaded'
          echo '- Docker resources cleaned up in test job'
        } >> $GITHUB_STEP_SUMMARY

  token-metrics:
    name: Report Token Usage Metrics
    runs-on: ubuntu-latest
    needs: [test, ai-review]
    if: always() && (needs.test.result == 'success' || needs.test.result == 'failure')

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for accurate git info

    - name: Download AI Review Token Data
      uses: actions/download-artifact@v4
      with:
        name: ai-review-token-data
        path: temp/
      continue-on-error: true

    - name: Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r active_interview_backend/requirements.txt

    - name: Run Database Migrations
      working-directory: active_interview_backend
      env:
        DJANGO_SECRET_KEY: ${{ secrets.DJANGO_SECRET_KEY }}
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      run: |
        python manage.py makemigrations --check --dry-run || python manage.py makemigrations
        python manage.py migrate --noinput

    - name: Show Token Files to Import
      run: |
        echo "Token files found:"
        ls -la temp/*.json 2>/dev/null || echo "No token files found"

    - name: Import Token Usage from CI/CD Scripts
      working-directory: .
      env:
        DJANGO_SECRET_KEY: ${{ secrets.DJANGO_SECRET_KEY }}
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      run: |
        python scripts/import-token-usage.py || echo "No token files to import"

    - name: Generate Token Metrics Report
      working-directory: .
      env:
        DJANGO_SECRET_KEY: ${{ secrets.DJANGO_SECRET_KEY }}
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        GITHUB_SHA: ${{ github.sha }}
        GITHUB_REF_NAME: ${{ github.ref_name }}
        GITHUB_BASE_REF: ${{ github.base_ref }}
        GITHUB_HEAD_REF: ${{ github.head_ref }}
        GITHUB_EVENT_NAME: ${{ github.event_name }}
      run: |
        python scripts/report-token-metrics.py | tee token-metrics-report.txt

    - name: Add Token Metrics to Summary
      if: always()
      run: |
        {
          echo '# ü§ñ Token Usage Metrics'
          echo ''
          echo '```'
          cat token-metrics-report.txt
          echo '```'
        } >> $GITHUB_STEP_SUMMARY

    - name: Upload Token Metrics Report
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: token-metrics-report
        path: token-metrics-report.txt
        retention-days: ${{ env.RETENTION_DAYS }}
